{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a short tutorial on how to implement and use spaCy's Entity Linking functionality. It can be used together with [this video](https://www.youtube.com/watch?v=8u57WSXVpmw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entity Linking** (EL) is the challenge of resolving ambiguous textual mentions to unique concepts in a knowledge base. A related task is **Named Entity Recognition** (NER). An NER component basically identifies words in text that have a specific name and refer to real-world objects, such as people or organizations. spaCy offers pre-built Machine Learning models that perform Named Entity Recognition for a variety of languages (https://spacy.io/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a  pretrained English model, apply it to some sample text and show the named entities that were identified by printing their text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity 'anxiety disorder' with label 'DISEASE'\n",
      "Named Entity 'generalized' with label 'DISEASE'\n",
      "Named Entity 'anxiety disorder' with label 'DISEASE'\n",
      "Named Entity 'anxiety anxiety' with label 'DISEASE'\n",
      "Named Entity 'generalized anxiety disorder' with label 'DISEASE'\n",
      "Named Entity 'anxiety' with label 'DISEASE'\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"../disease_ner/diseases-model\")\n",
    "text = \"\"\"alprazolam tablets are indicated for the management of \n",
    "    anxiety disorder a condition corresponding most closely to the \n",
    "    apa diagnostic and statistical manual dsm iii r diagnosis of generalized \n",
    "    anxiety disorder or the short term relief of symptoms of anxiety anxiety or \n",
    "    tension associated with the stress of everyday life usually does not require \n",
    "    treatment with an anxiolytic generalized anxiety disorder is characterized by \n",
    "    unrealistic or excessive anxiety and worry apprehensive expectation about two or \n",
    "    more life circumstances for a period of six months or longer during which the person \n",
    "    has been bothered more days than not by these concerns at least 6 of the following \"\"\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity 'hypertension' with label 'DISEASE'\n"
     ]
    }
   ],
   "source": [
    "text = \" hypertension \"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this sentence contains a person called \"Emerson\" and an organization called \"Wimbledon\". \n",
    "\n",
    "Unfortunately, there may be many people in the world called \"Emerson\", and this output still doesn't tell us which one exactly we meant. This is the challenge addressed by Entity Linking. It transforms an ambiguous textual mention to a unique identifier by looking at the context in which the mention occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific case, the sentence gives us important clues: Emerson is clearly a professional tennis player. \n",
    "\n",
    "Searching the internet, we can establish that this sentence is most likely talking about Roy Emerson, an Australian tennis player. We can now resolve this entity in this sentence to its unique identifier from WikiData, which is a free and open, interlingual knowledge base. Its unique IDs always start with a Q, and \"Roy Emerson\" has the identifier Q312545: https://www.wikidata.org/wiki/Q312545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement an entity linking pipeline, we need 3 different steps. \n",
    "\n",
    "The first step, as we already saw, is Named Entity Recognition, in which the mention \"Emerson\" is labeled as a \"Person\". Next, the extracted mention needs to be resolved to a list of plausible candidates. In our case, we'll consider three different people named Emerson. Typically, this list is created by querying a knowledge base (KB) that contains various aliases and synonyms. In the final step, we need to reduce the list of candidates to just one final ID that represents the correct Emerson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram of entity linking process](nel_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will show you how to use spaCy to create a Knowledge base that will address the second step of candidate generation. Additionally, we will create a new Entity Linking component, and train its Machine Learning model on some annotated data.\n",
    "\n",
    "The aim of this tutorial is to help you get started implementing your own Entity Linking functionality with spaCy. If you want to know more about the technical details, checkout this presentation at spaCy IRL 2019: https://www.youtube.com/watch?v=PW3RJM8tDGo&list=PLBmcuObd5An4UC6jvK_-eSl6jCvP1gwXc&index=7&t=0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Knowledge Base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to perform Entity Linking, is to set up a knowledge base that contains the unique identifiers of the entities we are interested in. In this tutorial we will create a very simple one with only 3 entries. We load the data from a pre-defined CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "entities_loc = Path.cwd() / \"mondo.obo\"\n",
    "term_start = False\n",
    "entity = {}\n",
    "with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "    for line in csvfile:\n",
    "        #print (line)\n",
    "        line = line.strip()\n",
    "        if \"[Term]\" in line:\n",
    "            term_start = True\n",
    "            continue\n",
    "        if term_start:\n",
    "            if \"id:\" in line:\n",
    "                did = line.replace('id: ','')\n",
    "                entity[did] ={'id':did}\n",
    "            if \"name:\" in line:\n",
    "                name = line.replace('name: ','')\n",
    "                entity[did]['name']=name\n",
    "            if \"def:\" in line:\n",
    "                def_ = line.replace('def: ','')\n",
    "                entity[did]['def']=def_\n",
    "                term_start = False\n",
    "            #print (entity)\n",
    "            #input(\"enter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23585"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MONDO:0000001',\n",
       "  {'id': 'MONDO:0000001',\n",
       "   'name': 'disease or disorder',\n",
       "   'def': '\"A disease is a disposition to undergo pathological processes that exists in an organism because of one or more disorders in that organism.\" [OGMS:0000031]'}),\n",
       " ('MONDO:0000002',\n",
       "  {'id': 'MONDO:0000002', 'name': 'obsolete 46,XX sex reversal'}),\n",
       " ('MONDO:0000003',\n",
       "  {'id': 'MONDO:0000003',\n",
       "   'name': 'obsolete 17-hydroxysteroid dehydrogenase deficiency'}),\n",
       " ('MONDO:0000004',\n",
       "  {'id': 'MONDO:0000004',\n",
       "   'name': 'adrenocortical insufficiency',\n",
       "   'def': '\"An endocrine or hormonal disorder that occurs when the adrenal cortex does not produce enough of the hormone cortisol and in some cases, the hormone aldosterone. It may be due to a disorder of the adrenal cortex (Addison\\'s disease or primary adrenal insufficiency) or to inadequate secretion of ACTH by the pituitary gland (secondary adrenal insufficiency).\" [NCIT:C26691]'}),\n",
       " ('MONDO:0000005', {'id': 'MONDO:0000005', 'name': 'alopecia, isolated'}),\n",
       " ('MONDO:0000006',\n",
       "  {'id': 'MONDO:0000006',\n",
       "   'name': 'obsolete alopecia-mental retardation syndrome'}),\n",
       " ('MONDO:0000007',\n",
       "  {'id': 'MONDO:0000007',\n",
       "   'name': 'obsolete atypical Mycobacteriosis, familial'}),\n",
       " ('MONDO:0000008',\n",
       "  {'id': 'MONDO:0000008', 'name': 'obsolete bare lymphocyte syndrome'}),\n",
       " ('MONDO:0000009',\n",
       "  {'id': 'MONDO:0000009',\n",
       "   'name': 'inherited bleeding disorder, platelet-type'}),\n",
       " ('MONDO:0000010',\n",
       "  {'id': 'MONDO:0000010',\n",
       "   'name': 'obsolete cerebrooculofacioskeletal syndrome'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(entity.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disease_entities.tsv', 'w') as fw:\n",
    "    for e in entity.values():\n",
    "        #print (e)\n",
    "        if 'def' in e  and 'name' in e:\n",
    "            fw.write(e['id']+'\\t'+e['name']+'\\t'+e['def']+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def load_entities():\n",
    "    entities_loc = Path.cwd() / \"disease_entities.tsv\"  # distributed alongside this notebook\n",
    "\n",
    "    names = dict()\n",
    "    descriptions = dict()\n",
    "    with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "        #next(csvreader)\n",
    "        for row in csvreader:\n",
    "            qid = row[0]\n",
    "            name = row[1]\n",
    "            desc = row[2]\n",
    "            names[qid] = name\n",
    "            descriptions[qid] = desc\n",
    "    return names, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict, desc_dict = load_entities()\n",
    "for QID in name_dict.keys():\n",
    "    print(f\"{QID}, name={name_dict[QID]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 entries here, of 3 different people called Emerson. One Australian tennis player, one American writer and one Brazilian footballer. We'll use this information to create our knowledge base. We need to define a fixed dimensionality for the entity vectors, which will be 300-D in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.kb import KnowledgeBase\n",
    "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add each record to the knowledge base, we encode its description using the built-in word vectors of our `nlp` model. The `vector` attribute of a document is the average of its token vectors. We also need to provide a frequency, which is a raw count of how many times a certain entity appears in an annotated corpus. In this tutorial we're not using these frequencies, so we're setting them to an arbitrary value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each enitity, decsription vectors were added \n",
    "# if you didn't download correct the tokens vector, you need to change vector length\n",
    "# if you download (python -m spacy download en_core_web_lg), the length should be 300\n",
    "for qid, desc in desc_dict.items():\n",
    "    desc_doc = nlp(desc)\n",
    "    desc_enc = desc_doc.vector\n",
    "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   # 342 is an arbitrary value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desc_doc = nlp('\"A disease is a disposition to undergo pathological processes that exists in an organism because of one or more disorders in that organism.\" [OGMS:0000031]')\n",
    "#kb.add_entity(entity='MONDO:0000001', entity_vector=desc_doc.vector, freq=342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to specify aliases or synonyms. We first add the full names. Here, we are 100% certain that they resolve to their corresponding QID, as there is no ambiguity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid, name in name_dict.items():\n",
    "    kb.add_alias(alias=name, entities=[qid], probabilities=[1])   # 100% prior probability P(entity|alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb.contains_entity('MONDO:0000001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MONDO:0000004', 'adrenocortical insufficiency'),\n",
       " ('MONDO:0000015', 'classic complement early component deficiency'),\n",
       " ('MONDO:0000022', 'nocturnal enuresis'),\n",
       " ('MONDO:0000044', 'hereditary hypophosphatemic rickets'),\n",
       " ('MONDO:0000062', 'isolated microphthalmia')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(name_dict.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.8/site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_annots(annot_file_name):\n",
    "    text_annots = {}\n",
    "    ann_arr =[]\n",
    "    with open(annot_file_name) as f:\n",
    "        for l in f:\n",
    "            res = json.loads(l)\n",
    "            text = res['text']\n",
    "            text_annots[text] = res\n",
    "    return text_annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = parse_annots('dailymed_disease3_L.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = []\n",
    "for text, res in annots.items():\n",
    "    t=  res['text']\n",
    "    for span in res['spans']:\n",
    "        s = span['start']\n",
    "        e = span['end']\n",
    "        entity_labels.append(t[s:e])\n",
    "        print (t[s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_labels = list(set(entity_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = name_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent2id= {v:k for k, v in name_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cand_gen(label, entities, ent2id, thres=70):\n",
    "    cands = process.extract(label, entities,scorer=fuzz.token_sort_ratio, limit=5)\n",
    "    total =0\n",
    "    for c,s in cands:\n",
    "        if s == 100: return [[ent2id[c]],[1.0]]\n",
    "        if s>thres:\n",
    "            total+=s\n",
    "            #print (c,s)\n",
    "    cands= [ (c,s/total) for c,s in cands if s>thres]\n",
    "    qids = [ ent2id[c] for c,s in cands ]\n",
    "    probs = [ s for c,s in cands ]\n",
    "    return qids, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pfeiffer syndrome 78\n",
      "Ehlers-Danlos syndrome 78\n",
      "Klinefelter syndrome 77\n",
      "CLOVES syndrome 76\n",
      "COFS syndrome 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Loeffler's syndrome\",\n",
       " (['MONDO:0007043',\n",
       "   'MONDO:0020066',\n",
       "   'MONDO:0006823',\n",
       "   'MONDO:0013038',\n",
       "   'MONDO:0008926'],\n",
       "  [0.203125, 0.203125, 0.20052083333333334, 0.19791666666666666, 0.1953125]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels[12], cand_gen(entity_labels[12], entities, ent2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acute frontal sinusitis 79\n",
      "acute maxillary sinusitis 72\n",
      "bacterial meningitis 71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('acute bacterial sinusitis',\n",
       " (['MONDO:0001912', 'MONDO:0002186', 'MONDO:0006670'],\n",
       "  [0.35585585585585583, 0.32432432432432434, 0.31981981981981983]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels[0], cand_gen(entity_labels[0], entities, ent2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart failure 74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('growth failure', (['MONDO:0005252'], [1.0]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_labels[1], cand_gen(entity_labels[1], entities, ent2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alzheimer disease 79\n",
      "Alzheimer disease 2 73\n",
      "Alzheimer disease 6 73\n",
      "Alzheimer disease 7 73\n",
      "Alzheimer disease 4 73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"Alzheimer's\",\n",
       " (['MONDO:0004975',\n",
       "   'MONDO:0007089',\n",
       "   'MONDO:0011561',\n",
       "   'MONDO:0011647',\n",
       "   'MONDO:0011743'],\n",
       "  [0.21293800539083557,\n",
       "   0.1967654986522911,\n",
       "   0.1967654986522911,\n",
       "   0.1967654986522911,\n",
       "   0.1967654986522911]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= \"Alzheimer's\"\n",
    "x, cand_gen(x, entities, ent2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "665"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-771630f86143>:7: UserWarning: [W017] Alias 'acute bacterial sinusitis' already exists in the Knowledge Base.\n",
      "  kb.add_alias(alias=flabel, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !\n",
      "<ipython-input-81-771630f86143>:7: UserWarning: [W017] Alias 'myelodysplastic/myeloproliferative diseases' already exists in the Knowledge Base.\n",
      "  kb.add_alias(alias=flabel, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !\n",
      "<ipython-input-81-771630f86143>:7: UserWarning: [W017] Alias 'VABP' already exists in the Knowledge Base.\n",
      "  kb.add_alias(alias=flabel, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !\n"
     ]
    }
   ],
   "source": [
    "aliases = {}\n",
    "words = []\n",
    "for flabel in entity_labels:\n",
    "    name = flabel\n",
    "    qids, probs = cand_gen(flabel, entities, ent2id)\n",
    "    if len(probs) ==1 and probs[0] ==1.0: continue\n",
    "    kb.add_alias(alias=flabel, entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to add the alias \"Emerson\". We'll assume that each of our 3 Emersons is equally famous and thus we set their probabilities to be equal for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qids = name_dict.keys()\n",
    "#probs = [0.3 for qid in qids]\n",
    "#kb.add_alias(alias=\"Emerson\", entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this will be our Knowledge base. We can check the entities and aliases that are contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
    "#print(f\"Aliases in the KB: {kb.get_alias_strings()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the candidates that are generated for the full name of Roy Emerson, as well as for the mention \"Emerson\" or for any other random mention, like \"Sofie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates for 'alzheimers': ['MONDO:0001105', 'MONDO:0005080', 'MONDO:0006875', 'MONDO:0015512', 'MONDO:0001134']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Candidates for 'alzheimers': {[c.entity_ for c in kb.get_candidates('hypertension')]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Candidates for 'Roy Stanley Emerson': {[c.entity_ for c in kb.get_candidates('Roy Stanley Emerson')]}\")\n",
    "print(f\"Candidates for 'Emerson': {[c.entity_ for c in kb.get_candidates('Emerson')]}\")\n",
    "print(f\"Candidates for 'Sofie': {[c.entity_ for c in kb.get_candidates('Sofie')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that querying the KB with the alias \"Emerson\" gives us 3 candidates, but if we query it with an unknown term, it just gives an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the knowledge base by calling the function `dump` with an output location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the directory and file names to whatever you like\n",
    "import os\n",
    "output_dir = Path.cwd() / \"output\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir) \n",
    "kb.dump(output_dir / \"my_kb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the `nlp` object to file by calling `to_disk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(output_dir / \"my_nlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create some annotated data to train an Entity Linking algorithm on. To do so, we will use the annotation tool Prodigy, but you could generate the data in whatever tool you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are watching [the video](https://www.youtube.com/watch?v=8u57WSXVpmw), it will explain how to obtain annotated data with Prodigy. The final result will be a JSONL file that is distributed alongside this notebook. We'll now use this JSONL file to train our entity linker. If you want to skip the annotation part in the video, you can fast forward to [this secion](https://www.youtube.com/watch?v=8u57WSXVpmw&t=19m19s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results in this file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We see that the full text of the original sentence is stored, together with a lot of detail about the annotation task. The most important bit is stored with the key `accept` at the end: this is the value of our manual annotation. For this specific sentence and this specific mention, the option with key `Q312545` was manually selected. This is the information that we'll train our entity linker on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed training data into our Entity Linker, we need to format our data as a structured tuple. The first part is the raw text, and the second part is a dictionary of annotations. This dictionary requires the `links` keyword, which holds another dictionary mapping a character offset to the correct identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = []\n",
    "json_loc = Path.cwd().parent / \"prodigy\" / \"emerson_annotated_text.jsonl\"\n",
    "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
    "    for line in jsonfile:\n",
    "        example = json.loads(line)\n",
    "        text = example[\"text\"]\n",
    "        if example[\"answer\"] == \"accept\":\n",
    "            QID = example[\"accept\"][0]\n",
    "            offset = (example[\"spans\"][0][\"start\"], example[\"spans\"][0][\"end\"])\n",
    "            links_dict = {QID: 1.0}\n",
    "        dataset.append((text, {\"links\": {offset: links_dict}}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the conversion looks OK, we can just print the first sample in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check some statistics in this dataset. How many cases of each QID do we have annotated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_ids = []\n",
    "for text, annot in dataset:\n",
    "    for span, links_dict in annot[\"links\"].items():\n",
    "        for link, value in links_dict.items():\n",
    "            if value:\n",
    "                gold_ids.append(link)\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(gold_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got exactly 10 annotated sentences for each of our Emersons. Of these, we'll now set aside 6 cases in a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_dataset = []\n",
    "test_dataset = []\n",
    "for QID in qids:\n",
    "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
    "    train_dataset.extend(dataset[index] for index in indices[0:8])  # first 8 in training\n",
    "    test_dataset.extend(dataset[index] for index in indices[8:10])  # last 2 in test\n",
    "    \n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our datasets now properly set up, we'll first run each of our training sentences through the pipeline with the NER component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DOCS = []\n",
    "for text, annotation in train_dataset:\n",
    "    doc = nlp(text)     # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
    "    TRAIN_DOCS.append((doc, annotation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll create a new Entity Linking component and add it to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_linker = nlp.create_pipe(\"entity_linker\", config={\"incl_prior\": False})\n",
    "entity_linker.set_kb(kb)\n",
    "nlp.add_pipe(entity_linker, last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will run the actual training loop for the new component, taking care to only train the entity linker and not the other components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\n",
    "with nlp.disable_pipes(*other_pipes):   # train only the entity_linker\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(500):   # 500 iterations takes about a minute to train\n",
    "        random.shuffle(TRAIN_DOCS)\n",
    "        batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  \n",
    "                annotations,   \n",
    "                drop=0.2,      # prevent overfitting\n",
    "                losses=losses,\n",
    "                sgd=optimizer,\n",
    "            )\n",
    "        if itn % 50 == 0:\n",
    "            print(itn, \"Losses\", losses)   # print the training loss\n",
    "print(itn, \"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training loss is pretty small, which is a good sign. But to truly verify whether our model generalizes well, we need to test it on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Entity Linker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first apply it on our original sentence. For each entity, we print the text and label as before, but also the disambiguated QID as predicted by our entity linker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_, ent.kb_id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Emerson gets disambiguated to Q312545, which is the correct ID for the tennis player. Note also that the entity \"Wimbledon\" gets the annotation `NIL`, which is basically just a placeholder value, showing that the NEL component could not find any relevant ID for this entity. This happens because our Knowledge base and the Entity Linking component have only been trained on \"Emerson\" examples, and are thus quite limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the model predicts for the 6 sentences in our test dataset, that were never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, true_annot in test_dataset:\n",
    "    print(text)\n",
    "    print(f\"Gold annotation: {true_annot}\")\n",
    "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
    "    for ent in doc.ents:\n",
    "        if ent.text == \"Emerson\":\n",
    "            print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results may vary a little from run to run, but usually the EL pipeline will get 5 out of 6 predictions correct (83% accuracy). Random guessing would have only achieved 33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this tutorial has shown you how to implement an Entity Linking component in spaCy. The knowledge base and training dataset used here were kept small for demonstration purposes, but in reality you'll want to use a much bigger representative set of entities, perhaps from an ontology or dictionary that is relevant to your use-case. \n",
    "\n",
    "If you have general questions on how to use this functionality in your own application, the best route is to create a new StackOverfow issue and tag it with the label `spaCy`. If you would run into an actual bug with the Entity Linking functionality, you can also open an issue at spaCy's github tracker. \n",
    "\n",
    "I hope your next NLP project will incorporate entity linking !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
